---
---

@article{klein2023generative,
    title={Generative Entropic Neural Optimal Transport To Map Within and Across Spaces}, 
    author={Dominik Klein* and Théo Uscidda* and Fabian Theis and Marco Cuturi},
    abstract={Learning measure-to-measure mappings is a crucial task in machine learning, featured prominently in generative modeling. Recent years have witnessed a surge of techniques that draw inspiration from optimal transport (OT) theory. Combined with neural network models, these methods collectively known as \textit{Neural OT} use optimal transport as an inductive bias: such mappings should be optimal w.r.t. a given cost function, in the sense that they are able to move points in a thrifty way, within (by minimizing displacements) or across spaces (by being isometric). This principle, while intuitive, is often confronted with several practical challenges that require adapting the OT toolbox: cost functions other than the squared-Euclidean cost can be challenging to handle, the deterministic formulation of Monge maps leaves little flexibility, mapping across incomparable spaces raises multiple challenges, while the mass conservation constraint inherent to OT can provide too much credit to outliers. While each of these mismatches between practice and theory has been addressed independently in various works, we propose in this work an elegant framework to unify them, called \textit{generative entropic neural optimal transport} (GENOT). GENOT can accommodate any cost function; handles randomness using conditional generative models; can map points across incomparable spaces, and can be used as an \textit{unbalanced} solver. We evaluate our approach through experiments conducted on various synthetic datasets and demonstrate its practicality in single-cell biology. In this domain, GENOT proves to be valuable for tasks such as modeling cell development, predicting cellular responses to drugs, and translating between different data modalities of cells.},
    year={2023},
    abbr={arXiv},
    journal={arXiv:2310.09254},
    pdf={https://arxiv.org/pdf/2310.09254.pdf},
    selected={true}
}

@article{eyring2023unbalancedness,
    title={Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation}, 
    author={Luca Eyring* and Dominik Klein* and Théo Uscidda* and Giovanni Palla and Niki Kilbertus and Zeynep Akata and Fabian Theis},
    abstract={In optimal transport (OT), a Monge map is known as a mapping that transports a source distribution to a target distribution in the most cost-efficient way. Recently, multiple neural estimators for Monge maps have been developed and applied in diverse unpaired domain translation tasks, e.g. in single-cell biology and computer vision. However, the classic OT framework enforces mass conservation, which makes it prone to outliers and limits its applicability in real-world scenarios. The latter can be particularly harmful in OT domain translation tasks, where the relative position of a sample within a distribution is explicitly taken into account. While unbalanced OT tackles this challenge in the discrete setting, its integration into neural Monge map estimators has received limited attention. We propose a theoretically grounded method to incorporate unbalancedness into any Monge map estimator. We improve existing estimators to model cell trajectories over time and to predict cellular responses to perturbations. Moreover, our approach seamlessly integrates with the OT flow matching (OT-FM) framework. While we show that OT-FM performs competitively in image translation, we further improve performance by incorporating unbalancedness (UOT-FM), which better preserves relevant features. We hence establish UOT-FM as a principled method for unpaired image translation.},
    booktitle={the 12th International Conference on Learning Representations},
    year={2024},
    abbr={ICLR},
    pdf={https://arxiv.org/pdf/2311.15100.pdf},
    selected={true}
}

@inproceedings{uscidda2023monge,
    title={The Monge Gap: A Regularizer to Learn All Transport Maps}, 
    author={Théo Uscidda and Marco Cuturi},
    abstract={Optimal transport (OT) theory has been been used in machine learning to study and characterize maps that can push-forward efficiently a probability measure onto another. Recent works have drawn inspiration from Brenier's theorem, which states that when the ground cost is the squared-Euclidean distance, the ``best'' map to morph a continuous measure in $\mathcal{P}(\Rd)$ into another must be the gradient of a convex function. To exploit that result, [Makkuva+ 2020, Korotin+2020] consider maps T=∇f_θ, where f_θ is an input convex neural network (ICNN), as defined by Amos+2017, and fit θ with SGD using samples. Despite their mathematical elegance, fitting OT maps with ICNNs raises many challenges, due notably to the many constraints imposed on θ; the need to approximate the conjugate of f_θ; or the limitation that they only work for the squared-Euclidean cost. More generally, we question the relevance of using Brenier's result, which only applies to densities, to constrain the architecture of candidate maps fitted on samples. Motivated by these limitations, we propose a radically different approach to estimating OT maps: Given a cost c and a reference measure ρ, we introduce a regularizer, the Monge gap M_ρ^c(T) of a map T. That gap quantifies how far a map T deviates from the ideal properties we expect from a c-OT map. In practice, we drop all architecture requirements for T and simply minimize a distance (e.g., the Sinkhorn divergence) between T♯μ and ν, regularized by M_ρ^c. We study M_ρ^c, and show how our simple pipeline outperforms significantly other baselines in practice.},
    booktitle={the 40th International Conference on Machine Learning},
    year={2023},
    abbr={ICML},
    pdf={https://proceedings.mlr.press/v202/uscidda23a/uscidda23a.pdf},
    selected={true}
}



